{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import path_a3d, path_cache\n",
    "from src.constants import BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3DScans(Dataset):\n",
    "    \"\"\"A3d scans data.\"\"\"\n",
    "\n",
    "    def __init__(self, labels=None, keep_label_idx=None, blacklist=None, transforms=None):\n",
    "        \"\"\"Initialize dataset with optional filters.\"\"\"\n",
    "        if labels is None:\n",
    "            labels = get_labels()\n",
    "        if keep_label_idx is not None:\n",
    "            labels = labels[labels.subject_id.isin(keep_label_idx)]\n",
    "        if blacklist is not None:\n",
    "            labels = labels[~labels.subject_id.isin(blacklist)]\n",
    "\n",
    "        # map to common zone (across left-right center) and filter\n",
    "        self.subject_ids = labels.subject_id.unique()\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get length of dataset.\"\"\"\n",
    "        return len(self.subject_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get data element at index `idx`.\"\"\"\n",
    "        # parse idx\n",
    "        subject_id = self.subject_ids[idx]\n",
    "#         image = tsa.read_data(path.join(path_a3d, subject_id + '.a3d'))\n",
    "        image = np.load(path.join(path_cache, subject_id + '.npy'))\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        return dict(image=image, subject_id=subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3d_scans = A3DScans()\n",
    "\n",
    "loader = DataLoader(\n",
    "        a3d_scans,\n",
    "        num_workers=4,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preprocess raw a3d scan images for use in model training.\"\"\"\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from os import path\n",
    "import json\n",
    "import tsahelper.tsahelper as tsa\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from src.config import path_a3d, path_cache, verbose, path_plots, path_logs\n",
    "from src.constants import IMAGE_DIM\n",
    "from src.utils import save_image, get_labels, moving_average\n",
    "\n",
    "\n",
    "def find_edges(a, buffer=0, plot_distr=False):\n",
    "    \"\"\"Find the edges of a series.\"\"\"\n",
    "    ma = moving_average(a, 10)\n",
    "    f = ma > 10000\n",
    "\n",
    "    _, lower = torch.max(f, 0)\n",
    "    _, upper = torch.max(reverse_tensor(f), 0)\n",
    "    lower = lower.data[0]\n",
    "    upper = f.size(0) - upper.data[0]\n",
    "    if f.data[0]:\n",
    "        lower = 0\n",
    "    if f.data[-1]:\n",
    "        upper = f.size(0)\n",
    "    return max(lower - buffer, 0), min(upper + buffer, f.size(0))\n",
    "\n",
    "\n",
    "def reverse_tensor(t):\n",
    "    \"\"\"Reverse tensor t.\"\"\"\n",
    "    dim = len(t.size()) - 1\n",
    "    tv = Variable(torch.LongTensor(range(t.size(dim) - 1, -1, -1)))\n",
    "    return t.index_select(dim, tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(img):\n",
    "    \"\"\"Rescale tensor image to range [0, 255].\"\"\"\n",
    "    min_, max_ = img.min(), img.max()\n",
    "    base_range = max_ - min_\n",
    "    rescaled_range = 255 - 0\n",
    "    return (img - min_) * rescaled_range / base_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(t, n):\n",
    "    \"\"\"Return the moving average series of a with kernel size n.\"\"\"\n",
    "    ma = nn.AvgPool1d(n - 1, stride=1, padding=4)\n",
    "    return ma(t.unsqueeze(1)).squeeze()\n",
    "\n",
    "\n",
    "def derivative(a, n):\n",
    "    \"\"\"Return the first derivative of series a with kernel size n.\"\"\"\n",
    "    return a - np.roll(a, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_edges(a, buffer=0, plot_distr=False):\n",
    "    \"\"\"Find the edges of a series.\"\"\"\n",
    "    ma = moving_average(a, 10)\n",
    "    f = ma > 10000\n",
    "\n",
    "    _, lower = torch.max(f, 1)\n",
    "    _, upper = torch.max(reverse_tensor(f), 1)\n",
    "\n",
    "    lower = lower.clamp(min=0, max=f.size(1))\n",
    "    upper = f.size(1) - upper.clamp(min=0, max=f.size(1)).type(torch.FloatTensor)\n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, buffer=0):\n",
    "    \"\"\"Find the edges of a TSA scan along each dimension and return the cropped image.\"\"\"\n",
    "    timg = rescale(Variable(image))\n",
    "    avg_pool = nn.AvgPool3d(2, 1, )\n",
    "    convolved = avg_pool(timg) * 2 ** 3  # convert to sum pool\n",
    "    convolved = convolved.squeeze()\n",
    "    filtered = (convolved * (convolved > 250).type(torch.DoubleTensor))\n",
    "\n",
    "    s0 = filtered.sum(dim=2).sum(dim=2)\n",
    "    s1 = filtered.sum(dim=1).sum(dim=2)\n",
    "    s2 = filtered.sum(dim=1).sum(dim=1)\n",
    "\n",
    "    # borders for each dimension\n",
    "    bottom, top = find_edges(s0, buffer)\n",
    "    left, right = find_edges(s1, buffer)\n",
    "    front, back = find_edges(s2, buffer)\n",
    "\n",
    "    resized_images = [\n",
    "        image[i, :int(t.data[0]), int(l.data[0]):int(r.data[0]), int(f.data[0]):int(b.data[0])]\n",
    "        for i, (t, l, r, f, b) in enumerate(zip(top, left, right, front, back))\n",
    "    ]\n",
    "    if verbose > 1:\n",
    "        print('Image resized from {} to {}'.format(image.shape, resized_image.shape))\n",
    "    return resized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690c0567ddd681f71652841feab25cea\n",
      "c744eca7a88ff6ccac47ae4bbc440b9f\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    cropped_images = crop_image(batch['image'])\n",
    "    for cropped_image, subject_id in zip(cropped_images, batch['subject_id']):\n",
    "        image = cropped_image.cpu().numpy()\n",
    "        print(subject_id)\n",
    "        resized_image = resize(image, (IMAGE_DIM, IMAGE_DIM, IMAGE_DIM), mode='constant')\n",
    "        np.save(path.join(path_cache, subject_id + '.npy'), resized_image)\n",
    "        crop_log[subject_id] = image.shape\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
